{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import copy\n",
    "import json\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# import ipywidgets as widgets\n",
    "# import bqplot.pyplot as bqplt\n",
    "# from tqdm.notebook import tqdm\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "# import tensorflow as tf\n",
    "# ['all', 'last', 'last_expr', 'none', 'last_expr_or_assign']\n",
    "InteractiveShell.ast_node_interactivity = \"last\"\n",
    "# matplotlib configuration\n",
    "mpl.rcParams['grid.color'] = 'k'\n",
    "mpl.rcParams['grid.linestyle'] = ':'\n",
    "mpl.rcParams['grid.linewidth'] = 0.5\n",
    "mpl.rcParams['font.size'] = 12\n",
    "# plt.style.use(['dark_background'])\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column 1: acceleration from the chest sensor (X axis)\n",
    "Column 2: acceleration from the chest sensor (Y axis)\n",
    "Column 3: acceleration from the chest sensor (Z axis)\n",
    "Column 4: electrocardiogram signal (lead 1) \n",
    "Column 5: electrocardiogram signal (lead 2)\n",
    "Column 6: acceleration from the left-ankle sensor (X axis)\n",
    "Column 7: acceleration from the left-ankle sensor (Y axis)\n",
    "Column 8: acceleration from the left-ankle sensor (Z axis)\n",
    "Column 9: gyro from the left-ankle sensor (X axis)\n",
    "Column 10: gyro from the left-ankle sensor (Y axis)\n",
    "Column 11: gyro from the left-ankle sensor (Z axis)\n",
    "Column 13: magnetometer from the left-ankle sensor (X axis)\n",
    "Column 13: magnetometer from the left-ankle sensor (Y axis)\n",
    "Column 14: magnetometer from the left-ankle sensor (Z axis)\n",
    "Column 15: acceleration from the right-lower-arm sensor (X axis)\n",
    "Column 16: acceleration from the right-lower-arm sensor (Y axis)\n",
    "Column 17: acceleration from the right-lower-arm sensor (Z axis)\n",
    "Column 18: gyro from the right-lower-arm sensor (X axis)\n",
    "Column 19: gyro from the right-lower-arm sensor (Y axis)\n",
    "Column 20: gyro from the right-lower-arm sensor (Z axis)\n",
    "Column 21: magnetometer from the right-lower-arm sensor (X axis)\n",
    "Column 22: magnetometer from the right-lower-arm sensor (Y axis)\n",
    "Column 23: magnetometer from the right-lower-arm sensor (Z axis)\n",
    "Column 24: Label (0 for the null class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['acc_chest_x', 'acc_chest_y', 'acc_chest_z', 'ecd_chest_1', 'ecd_chest_2',\n",
    "            'acc_lankle_x', 'acc_lankle_y', 'acc_lankle_z', 'gyro_lankle_x', 'gyro_lankle_y', 'gyro_lankle_z', 'mag_lankle_x', 'mag_lankle_y', 'mag_lankle_z',\n",
    "            'acc_rlarm_x', 'acc_rlarm_y', 'acc_rlarm_z', 'gyro_rlarm_x', 'gyro_rlarm_y', 'gyro_rlarm_z', 'mag_rlarm_x', 'mag_rlarm_y', 'mag_rlarm_z',\n",
    "            'label']\n",
    "\n",
    "\n",
    "class MHealthDataset:\n",
    "    def __init__(self, data_path, activities=None, train_rate=0.5, nb_views=5):\n",
    "        self.data_path = data_path\n",
    "        self.nb_views = nb_views\n",
    "        self.features = features\n",
    "        self.train_rate = train_rate\n",
    "        self.activities = activities\n",
    "\n",
    "    def load_data(self):\n",
    "        p_paths = sorted(glob.glob(self.data_path+\"/*.log\"))\n",
    "        p_dfs = [pd.read_csv(p_path, delimiter='\\t', names=self.features, header=None) for p_path in p_paths]\n",
    "        aps_dfs = {}\n",
    "        min_length = 100000\n",
    "        for p, p_df in enumerate(p_dfs):\n",
    "            tmp_dfs = [group[1] for group in p_df.groupby('label') if group[0] in self.activities]\n",
    "            for act in self.activities:\n",
    "                key = f\"a{act:02d}_p{p+1}\"\n",
    "                aps_dfs[key] = tmp_dfs[act-1]\n",
    "                min_length = min(min_length, aps_dfs[key].shape[0])\n",
    "        train_ap_dfs = {}\n",
    "        test_ap_dfs = {}\n",
    "        # train_split = int(min_length*self.train_rate)\n",
    "        train_split = 2096\n",
    "        for key, ap_df in aps_dfs.items():\n",
    "            # train_ap_dfs[key] = aps_dfs[key][:train_split].reset_index().drop(columns=['index'])\n",
    "            # test_ap_dfs[key] = aps_dfs[key][train_split:train_split+train_split].reset_index().drop(columns=['index'])\n",
    "            train_ap_dfs[key] = aps_dfs[key][:train_split].reset_index().drop(columns=['index'])\n",
    "            test_ap_dfs[key] = aps_dfs[key][:train_split].reset_index().drop(columns=['index'])\n",
    "        return train_ap_dfs, test_ap_dfs\n",
    "\n",
    "    def split_views(self, ap_dfs):\n",
    "        views = {}\n",
    "        for v, part in enumerate(['chest', 'lankle', 'rlarm']):\n",
    "            view_features = [feat for feat in self.features if part in feat]\n",
    "            views[f'view_{v+1}'] = {key: df[view_features]\n",
    "                                    for key, df in ap_dfs.items()}\n",
    "        return views\n",
    "\n",
    "data_path = '../raw_datasets/mhealth'\n",
    "activities = list(range(1, 11+1))\n",
    "clusters = [f\"a{act:02d}\" for act in activities]\n",
    "dataset = MHealthDataset(data_path, activities=activities)\n",
    "train_ap_dfs, test_ap_dfs = dataset.load_data()\n",
    "train_views_dfs = dataset.split_views(train_ap_dfs)\n",
    "test_views_dfs = dataset.split_views(test_ap_dfs)\n",
    "train_views_dfs['view_2']['a11_p1'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_views_dfs['view_2']['a11_p1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_dir = '../preprocessed_datasets/mhealth'\n",
    "# Train dataset\n",
    "for view, view_dfs in train_views_dfs.items():\n",
    "    view_path = stored_dir+f\"/raw/{view}\"\n",
    "    if not os.path.exists(view_path):\n",
    "        os.makedirs(view_path)\n",
    "    for ap, df in view_dfs.items():\n",
    "        for col in df.columns:\n",
    "            path = f\"{view_path}/{col}\"\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            df[col].to_csv(f\"{path}/{ap}.csv\", header=[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/anomaly_generations.py\n",
    "\n",
    "dataset_name = 'mhealth_timestep_same_subject_random_view'\n",
    "sample = 10\n",
    "nb_views = 3\n",
    "today = datetime.now().strftime(\"%Y%m%d\")\n",
    "for anomaly_rate in [5]:\n",
    "    dir_path = f\"../preprocessed_datasets/datasets_{today}/{dataset_name}/sample{sample}/anomaly_rate_{anomaly_rate}_views_{nb_views}\"\n",
    "    swapped_test_views_dfs, ground_truths = swap_time_steps(copy.deepcopy(\n",
    "        test_views_dfs), clusters=clusters, anomaly_rate=anomaly_rate*0.01)\n",
    "\n",
    "    # Save to files\n",
    "    print(\"Saving files...\")\n",
    "    for view, view_dfs in train_views_dfs.items():\n",
    "        view_path = dir_path+f\"/train/{view}\"\n",
    "        if not os.path.exists(view_path):\n",
    "            os.makedirs(view_path)\n",
    "        for ap, df in view_dfs.items():\n",
    "            if not 'a11' in ap:\n",
    "                df.to_csv(f\"{view_path}/{ap}.csv\", index=False)\n",
    "    for view, view_dfs in swapped_test_views_dfs.items():\n",
    "        view_path = dir_path+f\"/test/{view}\"\n",
    "        if not os.path.exists(view_path):\n",
    "            os.makedirs(view_path)\n",
    "        for ap, df in view_dfs.items():\n",
    "            if not 'a11' in ap:\n",
    "                df.to_csv(f\"{view_path}/{ap}.csv\", index=False)\n",
    "    for ap, gt in ground_truths.items():\n",
    "        if not 'a11' in ap:\n",
    "            gt.to_csv(dir_path+f\"/test/{ap}.csv\", index=False)\n",
    "    print('Done.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('mytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ee41a881656f5fc4dd86b207feac279397358c077bb0ebe254df13365e193ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
