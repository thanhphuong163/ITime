{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# import ipywidgets as widgets\n",
    "# import bqplot.pyplot as bqplt\n",
    "# from tqdm.notebook import tqdm\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "# import tensorflow as tf\n",
    "# ['all', 'last', 'last_expr', 'none', 'last_expr_or_assign']\n",
    "InteractiveShell.ast_node_interactivity = \"last\"\n",
    "# matplotlib configuration\n",
    "mpl.rcParams['grid.color'] = 'k'\n",
    "mpl.rcParams['grid.linestyle'] = ':'\n",
    "mpl.rcParams['grid.linewidth'] = 0.5\n",
    "mpl.rcParams['font.size'] = 12\n",
    "# plt.style.use(['dark_background'])\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DSA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run ../src/mex_loader.py\n",
    "\n",
    "features = ['act1', 'act2', 'act3', 'acw1', 'acw2', 'acw3']\n",
    "\n",
    "\n",
    "class MEXDataset:\n",
    "    def __init__(self, data_path, exercises=None, train_rate=0.5, nb_views=2):\n",
    "        self.data_path = data_path\n",
    "        self.nb_views = nb_views\n",
    "        self.features = features\n",
    "        self.train_rate = train_rate\n",
    "        self.exercises = exercises\n",
    "        self.views = ['act', 'acw']\n",
    "\n",
    "    def load_data(self):\n",
    "        train_views_dfs, test_views_dfs = {}, {}\n",
    "        min_length = 10000000\n",
    "        tmp_views_dfs = {}\n",
    "        for v, view in enumerate(self.views):\n",
    "            print(f'Loading {view}...')\n",
    "            p_paths = sorted(glob.glob(f\"{self.data_path}/{view}/*\"))\n",
    "            features = ['time'] + [feat for feat in self.features if view in feat]\n",
    "            tmp_views_dfs[f'view_{v+1}'] = {}\n",
    "            for p_path in p_paths:\n",
    "                p_key = p_path.split('/')[-1]\n",
    "                e_paths = [f\"{p_path}/{ex}_{view}_1.csv\" for ex in self.exercises]\n",
    "                dfs = {\n",
    "                    f\"e{ex}_p{p_key}\": pd.read_csv(path, names=features).drop(columns=['time']) \n",
    "                    for ex, path in zip(self.exercises, e_paths)\n",
    "                    }\n",
    "                min_length = min(min_length, *[df.shape[0] for df in dfs.values()])\n",
    "                for key, df in dfs.items():\n",
    "                    tmp_views_dfs[f'view_{v+1}'][key] = df\n",
    "        train_split = int(min_length*self.train_rate)\n",
    "        for view, view_dfs in tmp_views_dfs.items():\n",
    "            train_views_dfs[view], test_views_dfs[view] = {}, {}\n",
    "            for ep, df in view_dfs.items():\n",
    "                train_views_dfs[view][ep] = df[:min_length]\n",
    "                test_views_dfs[view][ep] = df[:min_length].reset_index().drop(columns=['index'])\n",
    "        return train_views_dfs, test_views_dfs\n",
    "    \n",
    "    def save_into_features(self, stored_dir, train_views_dfs):\n",
    "        print(\"Saving features into files\")\n",
    "        for view, view_dfs in train_views_dfs.items():\n",
    "            view_path = stored_dir+f\"/raw/{view}\"\n",
    "            if not os.path.exists(view_path):\n",
    "                os.makedirs(view_path)\n",
    "            for ap, df in view_dfs.items():\n",
    "                for col in df.columns:\n",
    "                    path = f\"{view_path}/{col}\"\n",
    "                    os.makedirs(path, exist_ok=True)\n",
    "                    df[col].to_csv(f\"{path}/{ap}.csv\", header=[col])\n",
    "\n",
    "\n",
    "exercises = ['01', '02', '03', '05', '06', '07']\n",
    "mex_dataset = MEXDataset(\"../raw_datasets/mex\", exercises=exercises)\n",
    "train_views_dfs, test_views_dfs = mex_dataset.load_data()\n",
    "stored_dir = '../preprocessed_datasets/mex'\n",
    "mex_dataset.save_into_features(stored_dir, train_views_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_scores(view, feature, instances):\n",
    "    score_path = f\"/home/tuangroup/ntphuong/projects/MVAD/preprocessed_datasets/mex/gen_data/{view}/{feature}_saved_scores.json\"\n",
    "    with open(score_path, 'r') as f:\n",
    "        saved_scores = json.load(f)\n",
    "    anomaly_scores_np = np.array([saved_score[1] for saved_score in saved_scores])\n",
    "    print(anomaly_scores_np.shape)\n",
    "    anomaly_scores = pd.DataFrame(anomaly_scores_np.T, columns=[f'{i}' for i in instances])\n",
    "    return anomaly_scores\n",
    "\n",
    "def plot_instance(instance, view, feature, instances):\n",
    "    anomaly_score = get_scores(view, feature, instances)\n",
    "    fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(20, 7*2))\n",
    "    data = train_views_dfs[view][instance][feature]\n",
    "    axs[0].plot(data)\n",
    "    predict_indices = anomaly_score[instance].sort_values(ascending=False)[:100].index\n",
    "    axs[0].scatter(predict_indices, data[predict_indices], c='r')\n",
    "    # new_data = data.copy(deep=True)\n",
    "    # for idx in predict_indices:\n",
    "    #     new_data[idx] = 0.\n",
    "    # axs[1].plot(new_data)\n",
    "    axs[1].plot(anomaly_score[instance])\n",
    "    plt.show()\n",
    "\n",
    "def replace_anomaly_values(view, feature, instances):\n",
    "    anomaly_scores = get_scores(view, feature, instances)\n",
    "    for instance in instances:\n",
    "        predict_indices = anomaly_scores[instance].sort_values(ascending=False)[:20].index\n",
    "        for idx in predict_indices:\n",
    "            train_views_dfs[view][instance][feature][idx] = np.nan\n",
    "            test_views_dfs[view][instance][feature][idx] = np.nan\n",
    "        # train_views_dfs[view][instance].fillna(method='ffill', inplace=True)\n",
    "        # test_views_dfs[view][instance].fillna(method='ffill', inplace=True)\n",
    "        # NOTE: We use linear method since it is the only method that supports MultiIndexes\n",
    "        train_views_dfs[view][instance].interpolate(method='linear', inplace=True)\n",
    "        test_views_dfs[view][instance].interpolate(method='linear', inplace=True)\n",
    "        \n",
    "\n",
    "\n",
    "# instance = 'a01_p10'\n",
    "# view = 'view_2'\n",
    "# feature = 'mag_lankle_y'\n",
    "# instances = sorted(list(train_views_dfs['view_1'].keys()))\n",
    "# plot_instance(instance, view, feature, instances)\n",
    "# replace_anomaly_values(view, feature, instances)\n",
    "# plot_instance(instance, view, feature, instances)\n",
    "for view, view_dfs in train_views_dfs.items():\n",
    "    instances = sorted(list(train_views_dfs[view].keys()))\n",
    "    for feature in view_dfs[instances[0]].columns:\n",
    "        replace_anomaly_values(view, feature, instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for view in train_views_dfs.keys():\n",
    "    for instance in train_views_dfs['view_1'].keys():\n",
    "        split_point = train_views_dfs[view][instance].shape[0] // 2\n",
    "        train_views_dfs[view][instance] = pd.DataFrame(train_views_dfs[view][instance][:split_point].values, columns=train_views_dfs[view][instance].columns)\n",
    "        test_views_dfs[view][instance] = pd.DataFrame(test_views_dfs[view][instance][split_point:].values, columns=test_views_dfs[view][instance].columns)\n",
    "print(train_views_dfs['view_1']['e01_p01'].shape)\n",
    "print(test_views_dfs['view_1']['e01_p01'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/anomaly_generations.py\n",
    "\n",
    "dataset_name = 'mex_timestep_same_subject_random_view'\n",
    "n_samples = 15\n",
    "nb_views = 2\n",
    "\n",
    "for sample in range(6, n_samples+1):\n",
    "    for anomaly_rate in [5, 10, 15, 20]:\n",
    "        clusters = [f'e{ex}' for ex in exercises]\n",
    "        dir_path = f\"../preprocessed_datasets/{dataset_name}/sample{sample}/anomaly_rate_{anomaly_rate}_views_{nb_views}\"\n",
    "        swapped_test_views_dfs, ground_truths = swap_time_steps(copy.deepcopy(\n",
    "            test_views_dfs), clusters=clusters, anomaly_rate=anomaly_rate*0.01)\n",
    "\n",
    "        # Save to files\n",
    "        print(\"Saving files...\")\n",
    "        for view, view_dfs in train_views_dfs.items():\n",
    "            view_path = dir_path+f\"/train/{view}\"\n",
    "            if not os.path.exists(view_path):\n",
    "                os.makedirs(view_path)\n",
    "            for ap, df in view_dfs.items():\n",
    "                df.to_csv(f\"{view_path}/{ap}.csv\", index=False)\n",
    "        for view, view_dfs in swapped_test_views_dfs.items():\n",
    "            view_path = dir_path+f\"/test/{view}\"\n",
    "            if not os.path.exists(view_path):\n",
    "                os.makedirs(view_path)\n",
    "            for ap, df in view_dfs.items():\n",
    "                df.to_csv(f\"{view_path}/{ap}.csv\", index=False)\n",
    "        for ap, gt in ground_truths.items():\n",
    "            gt.to_csv(dir_path+f\"/test/{ap}.csv\", index=False)\n",
    "        print('Done.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ee41a881656f5fc4dd86b207feac279397358c077bb0ebe254df13365e193ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
